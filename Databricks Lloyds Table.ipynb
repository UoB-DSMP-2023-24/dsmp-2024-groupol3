{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753037f3-ccce-4665-b85b-e2efab741bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Raw Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/fake_transactional_data_24__1_.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df_check = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Register DataFrame as a temporary view\n",
    "df_check.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ed5666-68ea-4b45-bed0-690bab576927",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### First column check - account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92328e48-4990-40c4-9dd6-17849d1d82f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query to check if all the entries end in .0\n",
    "SELECT\n",
    "  from_totally_fake_account\n",
    "FROM\n",
    "  temp_table\n",
    "WHERE\n",
    "  from_totally_fake_account NOT LIKE '%.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96958d93-9957-44ff-8175-0e5ac7a075cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Second column check - random generated account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505faf37-92e5-4dd8-9eec-d8d7b648c652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  DISTINCT\n",
    "  from_totally_fake_account,\n",
    "  to_randomly_generated_account\n",
    "FROM\n",
    "  temp_table\n",
    "ORDER BY\n",
    "  from_totally_fake_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c50f69-8e78-43ca-88f5-f2e072440438",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Focus on a single account \n",
    "SELECT\n",
    "  from_totally_fake_account,\n",
    "  to_randomly_generated_account\n",
    "FROM\n",
    "  temp_table\n",
    "WHERE\n",
    " from_totally_fake_account = '1000.0'\n",
    "ORDER BY\n",
    "  to_randomly_generated_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8360b1-1122-466e-91b8-f0ceca5ba855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Curated Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4f207a-f4a3-40b3-8533-887a8abb4884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower, regexp_replace\n",
    "\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/fake_transactional_data_24__1_.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Changing the column names\n",
    "new_column_names = [\"account_id\", \"purchase_amount\", \"purchase_type\", \"date_id\"]\n",
    "df = df.toDF(*new_column_names)\n",
    "\n",
    "# Check if all values in 'account_id' end with '.0' and remove decimal part\n",
    "df = df.withColumn(\"account_id_whole\", \n",
    "                   when(col(\"account_id\").substr(-2, 2) == \".0\", \n",
    "                        col(\"account_id\").cast(\"integer\")).otherwise(col(\"account_id\")))\n",
    "df = df.drop(\"account_id\").withColumnRenamed(\"account_id_whole\", \"account_id\")\n",
    "\n",
    "# Convert purchase amount column in to a float\n",
    "df = df.withColumn(\"purchase_amount\", df[\"purchase_amount\"].cast(\"float\"))\n",
    "\n",
    "# Cleaning the purchase type column so its consistent\n",
    "# Removing _ and replacing with spaces and lowercasing all text\n",
    "df = df.withColumn(\"purchase_type\",\n",
    "                   when(df[\"purchase_type\"].rlike(\"[A-Z_]\"),\n",
    "                        lower(regexp_replace(df[\"purchase_type\"], \"_\", \" \"))).otherwise(df[\"purchase_type\"]))\n",
    "\n",
    "# Reorder columns\n",
    "df = df.select(\"account_id\", \"purchase_amount\", \"purchase_type\", \"date_id\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view or table\n",
    "\n",
    "temp_table_name = \"lloyds_raw_data\"\n",
    "\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/* Query the created temp table in a SQL cell */\n",
    "\n",
    "select * from `lloyds_raw_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c865542-ee6b-4178-ba4f-39309f81261f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS lloyds_raw_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n",
    "# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n",
    "\n",
    "# Removes the table if its stored - Error handling\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/lloyds_raw_data_table/\", True)\n",
    "\n",
    "# Saves the new table\n",
    "raw_transactional_data = \"lloyds_raw_data_table\"\n",
    "df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(raw_transactional_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48227af-5ce7-446b-8c4e-183691056df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1631703378154663,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Lloyds Table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
